{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 6: Creating Validation Data.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "JndnmDMp66FL"
      ]
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "JndnmDMp66FL"
      },
      "cell_type": "markdown",
      "source": [
        "#### Copyright 2017 Google LLC."
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "hMqWDc_m6rUC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4f3CKqFUqL2-",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# Lab 6: Creating Validation Data\n",
        "**Learning Objectives:**\n",
        "  * Generate a train and validation data set for housing data that we will use to predict the median housing price, at the granularity of city blocks.\n",
        "  * Debug issues in the creation of the train and validation splits.\n",
        "  * Select the best single feature to use to train a linear model to predict the median housing price.\n",
        "  * Test that the prediction loss on the validation data accurately reflect the trained model's loss on unseen test data."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UHU_WK5Z2gsw"
      },
      "cell_type": "markdown",
      "source": [
        "### Standard Set-up\n",
        "\n",
        "We begin with the standard set-up.  In this lab we use a data set based on 1990 census data from California. Since this data set has a header row, we don't need to provide the column names."
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "P5_-Nmc52gsy",
        "slideshow": {
          "slide_type": "-"
        },
        "colab": {
          "test": {
            "output": "ignore",
            "timeout": 600
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.learn.python.learn import learn_io, estimator\n",
        "\n",
        "# This line increases the amount of logging when there is an error. You can\n",
        "# remove it if you want less logging.\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "# Set the output display to have two digits for decimal places, for display\n",
        "# readability only and limit it to printing 15 rows.\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "pd.options.display.max_rows = 15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "RG4WjoRq7we3"
      },
      "cell_type": "markdown",
      "source": [
        "Read the data set."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WrtTmerb7wOT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "california_housing_dataframe = pd.read_csv(\"https://storage.googleapis.com/ml_universities/california_housing_train.csv\", sep=\",\")\n",
        "california_housing_dataframe.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "K83U9tm72yF7"
      },
      "cell_type": "markdown",
      "source": [
        "##Prepare Features\n",
        "\n",
        "As our learning models get more sophisticated, we will want to do some computation on the features and even generate new features from the existing features. We see examples of this in later labs. For now this method will just make a copy of the portion of the dataframe we plan to use, and re-scale the median-house value (to make it a bit easier to work with)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JETiyZLS2gtY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepare_features(dataframe):\n",
        "  \"\"\"Prepares the features for the provided dataset.\n",
        "\n",
        "  Args:\n",
        "    dataframe: A Pandas DataFrame containing the data set.\n",
        "  Returns:\n",
        "    A new DataFrame that contains the features to be used for the model.\n",
        "  \"\"\"\n",
        "  processed_features = dataframe.copy()\n",
        "  \n",
        "  # Modifying median_house_value to be in scale of $1000.  So a value of 14.0\n",
        "  # will correspond to $14,000. This will make it a bit easier to work with.\n",
        "  processed_features[\"median_house_value\"] /= 1000.0\n",
        "  \n",
        "  return processed_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "91yJ6WebbC57"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Standard Functions to Train and Evaluate a Linear Regression Model\n",
        "\n",
        "As part of this lab you will train linear regression model to predict the median home price from the median family income. We copy all of the functions needed to do this from the previous labs so you have them available to use later in this lab."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "G2F3zbi2CjM7"
      },
      "cell_type": "markdown",
      "source": [
        "### Compute Loss\n",
        "\n",
        "Here is a simple method to compute the loss on the given input function and targets."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sPHDJH0JCiWP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_loss(model, input_fn, targets):\n",
        "  \"\"\" Computes the loss (RMSE) for linear regression.\n",
        "  \n",
        "  Args:\n",
        "    model: the trained model to use for making the predictions\n",
        "    input_fn: the input_fn to use to make the predictions\n",
        "    targets: a list of the target values being predicted that must be the\n",
        "             same length as predictions.\n",
        "    \n",
        "  Returns:\n",
        "    The RMSE for the provided predictions and targets.\n",
        "  \"\"\"      \n",
        "  predictions = list(model.predict(input_fn=input_fn))\n",
        "  return math.sqrt(metrics.mean_squared_error(predictions, targets))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2BokkZ4g2gs4"
      },
      "cell_type": "markdown",
      "source": [
        "### Setting Up the Feature Columns and Input Function for TensorFlow\n",
        "We create a list of the categorical and numerical features that we will use for training our model. Recall that it's okay if one of these lists is empty. In this lab in addition to having a training set, we introduce a validation set that will be used to select features and tune the hyperparameters used for training.  There's also a test data set representing the unseen data that we want the model to generalize to perform well. To be able to train or just evaluate a model for these data sets, we define `train_input_fn` to use the training data, `eval_input_fn` to use the validation data, and `test_input_fn` to use the test data."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OwnMzAkn2gs6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CATEGORICAL_COLUMNS = []\n",
        "NUMERICAL_COLUMNS = [\"longitude\", \"latitude\", \"housing_median_age\", \n",
        "                     \"total_rooms\", \"total_bedrooms\", \"population\",\n",
        "                     \"households\", \"median_income\", \"median_house_value\"]\n",
        "def input_fn(dataframe):\n",
        "  \"\"\"Constructs a dictionary for the feature columns.\n",
        "\n",
        "  Args:\n",
        "    dataframe: The Pandas DataFrame to use for the input.\n",
        "  Returns:\n",
        "    The feature columns and the associated labels for the provided input.\n",
        "  \"\"\"\n",
        "  # Creates a dictionary mapping each numeric feature column name (k) to\n",
        "  # the values of that column stored in a constant Tensor.\n",
        "  numerical_cols = {k: tf.constant(dataframe[k].values)\n",
        "                     for k in NUMERICAL_COLUMNS}\n",
        "  # Creates a dictionary mapping each categorical feature column name (k)\n",
        "  # to the values of that column stored in a tf.SparseTensor.\n",
        "  categorical_cols = {k: tf.SparseTensor(\n",
        "      indices=[[i, 0] for i in range(dataframe[k].size)],\n",
        "      values=dataframe[k].values,\n",
        "      dense_shape=[dataframe[k].size, 1])\n",
        "                      for k in CATEGORICAL_COLUMNS}\n",
        "  # Merges the two dictionaries into one.\n",
        "  feature_cols = dict(numerical_cols.items() + categorical_cols.items())\n",
        "  # Converts the label column into a constant Tensor.\n",
        "  label = tf.constant(dataframe[LABEL].values)\n",
        "  # Returns the feature columns and the label.\n",
        "  return feature_cols, label\n",
        "\n",
        "def train_input_fn():\n",
        "  return input_fn(training_examples)\n",
        "\n",
        "def eval_input_fn():\n",
        "  return input_fn(validation_examples)\n",
        "\n",
        "def test_input_fn():\n",
        "  return input_fn(test_examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "v5XS0Z_-GevD"
      },
      "cell_type": "markdown",
      "source": [
        "### Functions to help visualize our results\n",
        "\n",
        "As in past labs, we define functions to generate a calibration plot and learning curve with the change that the learning curve will include both the training loss and validation loss in order to help visually see when we are starting to overfit the data."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YIXTkIa6GB0s",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_calibration_plot(predictions, targets):\n",
        "  \"\"\" Creates a calibration plot.\n",
        "  \n",
        "  Args:\n",
        "    predictions: a list of values predicted by the model.\n",
        "    targets: a list of the target values being predicted that must be the\n",
        "             same length as predictions.\n",
        "  \"\"\"  \n",
        "  calibration_data = pd.DataFrame()\n",
        "  calibration_data[\"predictions\"] = pd.Series(predictions)\n",
        "  calibration_data[\"targets\"] = pd.Series(targets)\n",
        "  calibration_data.describe()\n",
        "  min_val = calibration_data[\"predictions\"].min()\n",
        "  max_val = calibration_data[\"predictions\"].max()\n",
        "  plt.ylabel(\"target\")\n",
        "  plt.xlabel(\"prediction\")\n",
        "  plt.scatter(predictions, targets, color='black')\n",
        "  plt.plot([min_val, max_val], [min_val, max_val])\n",
        "  \n",
        "def plot_learning_curve(training_losses, validation_losses):\n",
        "  \"\"\" Plot the learning curve.\n",
        "  \n",
        "  Args:\n",
        "    training_loses: a list of losses to plot.\n",
        "    validation_losses: a list of validation losses to plot.\n",
        "  \"\"\"        \n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Training Steps')\n",
        "  plt.plot(training_losses, label=\"training\")\n",
        "  plt.plot(validation_losses, label=\"validation\")\n",
        "  plt.legend(loc=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MD--C-wVGEEg"
      },
      "cell_type": "markdown",
      "source": [
        "### Defining the features\n",
        "\n",
        "\n",
        "This data set only has numerical features.  As a starting point we will introduce one `real_valued_column` for each feature we want to use in predicting the `median_house_value`.  In the below code, we have set this to `households`, however, you are encouraged to switch this to a feature you think might be more relevant to predict the median house value."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uq6e_T_1F94U",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NUMERICAL_FEATURES = [\"households\"]\n",
        "LABEL = \"median_house_value\"\n",
        "\n",
        "def construct_feature_columns():\n",
        "  \"\"\"Construct TensorFlow Feature Columns for the given features.\n",
        "  \n",
        "  Returns:\n",
        "    A set of feature columns.\n",
        "  \"\"\"\n",
        "  feature_set = set([tf.contrib.layers.real_valued_column(feature) \n",
        "                     for feature in NUMERICAL_FEATURES])\n",
        "  return feature_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7PvlEInX2gtL"
      },
      "cell_type": "markdown",
      "source": [
        "### Functions for defining the linear regression model and training it\n",
        "\n",
        "We slightly modify our function to train a model to also store the validation loss so that we can include it on our learning curve. We use a calibration plot as a way of visualizing the model."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "n7WDRxUo2gtN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def define_linear_regression_model(learning_rate):\n",
        "  \"\"\" Defines a linear regression model of one feature to predict the target.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: A `float`, the learning rate.\n",
        "    \n",
        "  Returns:\n",
        "    A linear regressor created with the given parameters.\n",
        "  \"\"\"\n",
        "  linear_regressor = tf.contrib.learn.LinearRegressor(\n",
        "    feature_columns=construct_feature_columns(),\n",
        "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate),\n",
        "    gradient_clip_norm=5.0\n",
        "  )  \n",
        "  return linear_regressor\n",
        "\n",
        "def train_model(linear_regressor, steps):\n",
        "  \"\"\"Trains a linear regression model.\n",
        "  \n",
        "  Args:\n",
        "    linear_regressor: The regressor to train.\n",
        "    steps: A non-zero `int`, the total number of training steps.\n",
        "    \n",
        "  Returns:\n",
        "    The trained regressor.\n",
        "  \"\"\"\n",
        "  # In order to see how the model evolves as we train it, we divide the\n",
        "  # steps into periods and show the model after each period.\n",
        "  periods = 10\n",
        "  steps_per_period = steps / periods\n",
        "  \n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.  We store the training and validation losses so we can\n",
        "  # generate a learning curve.\n",
        "  print \"Training model...\"\n",
        "  training_losses = []\n",
        "  validation_losses = []\n",
        "\n",
        "  for period in range (0, periods):\n",
        "    # Call fit to train the regressor for steps_per_period steps.\n",
        "    linear_regressor.fit(input_fn=train_input_fn, steps=steps_per_period)\n",
        "    \n",
        "    # Compute the loss between the predictions and the correct labels, append\n",
        "    # the training and validation loss to the list of losses used to generate\n",
        "    # the learning curve after training is complete and print the current\n",
        "    # training loss.\n",
        "    training_loss = compute_loss(linear_regressor, train_input_fn,\n",
        "                                 training_examples[LABEL])\n",
        "    validation_loss = compute_loss(linear_regressor, eval_input_fn,\n",
        "                                   validation_examples[LABEL])\n",
        "    training_losses.append(training_loss) \n",
        "    validation_losses.append(validation_loss) \n",
        "    print \"  Training loss after period %02d : %0.3f\" % (period, training_loss)\n",
        "      \n",
        "  # Now that training is done print the final training and validation losses.    \n",
        "  print \"Final Training Loss (RMSE): %0.3f\" % training_loss\n",
        "  print \"Final Validation Loss (RMSE): %0.3f\" % validation_loss \n",
        "  \n",
        "  # Generate a figure with the learning curve on the left and a\n",
        "  # calibration plot on the right.\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.title(\"Learning Curve (RMSE vs time)\")\n",
        "  plot_learning_curve(training_losses, validation_losses)\n",
        "  \n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.tight_layout(pad=1.1, w_pad=3.0, h_pad=3.0) \n",
        "  plt.title(\"Calibration Plot on Validation Data\")\n",
        "  validation_predictions = np.array(list(linear_regressor.predict(\n",
        "      input_fn=eval_input_fn)))\n",
        "  make_calibration_plot(validation_predictions, validation_examples[LABEL])\n",
        "   \n",
        "  return linear_regressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "O4tSQbQmGo7m"
      },
      "cell_type": "markdown",
      "source": [
        "##Divide the provided data for training our model into a training and validation set\n",
        "\n",
        "Our goal for training a model is to make predictions on new unseen data.  As the model gets larger (in terms of the number of weights we are learning), it is possible to start memorizing the training data and overfitting noise that might be in that data.  When overfitting occurs our model will make poor predictions on new data which defeats our purpose.  Thus we need a mechanism to recongize when overfitting occurs.  A common way todo this is to set aside some of the training data as a validation set using the rest as our training set.\n",
        "\n",
        "For the ***training set***, we'll choose the first 14000 examples, out of the total of 17000."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pr9J6wVUGoQX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_examples = prepare_features(california_housing_dataframe.head(14000))\n",
        "training_examples.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eol0w6KGHJbW"
      },
      "cell_type": "markdown",
      "source": [
        "For the ***validation set***, we'll choose the last 3000 examples, out of the total of 17000."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bxkg7NVPHJ_X",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "validation_examples = prepare_features(california_housing_dataframe.tail(3000))\n",
        "validation_examples.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MKHVqY0a4goT"
      },
      "cell_type": "markdown",
      "source": [
        "### Examine the data\n",
        "\n",
        "Let's take a close look at two features in particular: **`latitude`** and **`longitude`**. These are geographical coordinates of the city block in question.\n",
        "\n",
        "This might make a nice visualization — let's plot `latitude` and `longitude`, and use color to show the `median_house_value`."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_RRnuoeq4Su3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(13, 8))\n",
        "\n",
        "ax = plt.subplot(1, 2, 1)\n",
        "ax.set_title(\"Training Data\")\n",
        "ax.set_autoscaley_on(False)\n",
        "ax.set_ylim([32, 43])\n",
        "ax.set_autoscalex_on(False)\n",
        "ax.set_xlim([-126, -112])\n",
        "plt.scatter(training_examples[\"longitude\"],\n",
        "            training_examples[\"latitude\"],\n",
        "            cmap=\"coolwarm\",\n",
        "            c=training_examples[\"median_house_value\"] / training_examples[\"median_house_value\"].max())\n",
        "\n",
        "ax = plt.subplot(1, 2, 2)\n",
        "ax.set_title(\"Validation Data\")\n",
        "\n",
        "ax.set_autoscaley_on(False)\n",
        "ax.set_ylim([32, 43])\n",
        "ax.set_autoscalex_on(False)\n",
        "ax.set_xlim([-126, -112])\n",
        "plt.scatter(validation_examples[\"longitude\"],\n",
        "            validation_examples[\"latitude\"],\n",
        "            cmap=\"coolwarm\",\n",
        "            c=validation_examples[\"median_house_value\"] / validation_examples[\"median_house_value\"].max())\n",
        "\n",
        "_ = plt.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "oj2N-bR7OcKP"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 1: Train a Model (1 point)\n",
        "\n",
        "Pick the single feature that you think would lead to the best model to prdict the `median_house_value`. Adjust the learning_rate and steps to train a good model.  HINT: you should be able to get the RMSE down to around 80.  Using `households` is not a good choice! Look at what other features are available."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_QscSNCVctHb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NUMERICAL_FEATURES = [\"households\"]\n",
        "LABEL = \"median_house_value\"\n",
        "\n",
        "\n",
        "LEARNING_RATE = 0.005\n",
        "STEPS = 50\n",
        "\n",
        "linear_regressor = define_linear_regression_model(learning_rate = LEARNING_RATE)\n",
        "linear_regressor = train_model(linear_regressor, steps=STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "27I6NhglBnUB"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the Provided Test Data\n",
        "\n",
        "This data set (as with many) comes with a provided test data set that is representative and is used to evaluate the final performance. For this data set, the provided test data is located [here](https://storage.googleapis.com/ml_universities/california_housing_test.csv).\n",
        "\n",
        "The purpose of having validation data is to notice overfitting and other problems. Remember our key goal is to train a model that will make good predictions on **new unseen data**. Remember that the test data should only be used at the end to see how your final model is performing.  It should not be used in helping select which features to use or to select hyperparameter values."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XK2--38hBnl3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "california_housing_test_data = pd.read_csv(\n",
        "    \"https://storage.googleapis.com/ml_universities/california_housing_test.csv\",\n",
        "    sep=\",\")\n",
        "\n",
        "test_examples = prepare_features(california_housing_test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "es7XHoyUOU36"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 2: Measure the Test Error (1/2 point)\n",
        "\n",
        "Modify the codebox below to measure the test error. Look at the training error, validation error, and test error for the model you trained in Task 1.  You should only do this after you have picked your hyperparameters."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "z6ZgKEIoNf1l",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "g_YOLbzj4oYb"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 3: Recognize the Problem in the Splitting of the Data (1 point)\n",
        "There's something that we forgot to do above and it is going to cause a problem.  You need to figure out what that is and fix it before you can move on.  Below is some guidance to help you recognize there is a problem and from there you should be able to think through the issue and figure out the cause and what you can do to correct it.\n",
        "\n",
        "We should see something that resembles a map of California, with red showing up in expensive areas like the San Francisco and Los Angeles. The training set sort of does, but the validation data does not.  Answer the following questions directly in the comment area of the codebox below."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "f4umODuWlxv0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "A) Do you see any other differences in the distributions of features or targets\n",
        "   between the training and validation data?\n",
        "  \n",
        "B) Why is this happening?\n",
        "  \n",
        "C) How is this problem reflected when you look at the relationship between the\n",
        "   training error, validation error and test error?\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gvwyMQ-VPUJy"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 4: Fix the Problem (1 point)\n",
        "\n",
        "Make the changes here to how the training and validation examples are created and call prepare features again. (1 point)\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "AO4yWnEbK4id",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Add what you need to this code block to fix the issue you have seen ##\n",
        "\n",
        "\n",
        "## Regenerate the training and validation examples -- put your changes here\n",
        "\n",
        "training_examples = prepare_features(california_housing_dataframe.head(14000))\n",
        "validation_examples = prepare_features(california_housing_dataframe.tail(3000))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PGZoIu9DPkOl"
      },
      "cell_type": "markdown",
      "source": [
        "####Here we again view the map view of the data to confirm if the issue has been resolved.  Does this look different than above?  If not, you still have not fixed the issue.  It should be very clear once you have."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pMqfWAe-8xL-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(13, 8))\n",
        "\n",
        "ax = plt.subplot(1, 2, 1)\n",
        "ax.set_title(\"Training Data\")\n",
        "ax.set_autoscaley_on(False)\n",
        "ax.set_ylim([32, 43])\n",
        "ax.set_autoscalex_on(False)\n",
        "ax.set_xlim([-126, -112])\n",
        "plt.scatter(training_examples[\"longitude\"],\n",
        "            training_examples[\"latitude\"],\n",
        "            cmap=\"coolwarm\",\n",
        "            c=training_examples[\"median_house_value\"] / training_examples[\"median_house_value\"].max())\n",
        "\n",
        "ax = plt.subplot(1, 2, 2)\n",
        "ax.set_title(\"Validation Data\")\n",
        "\n",
        "ax.set_autoscaley_on(False)\n",
        "ax.set_ylim([32, 43])\n",
        "ax.set_autoscalex_on(False)\n",
        "ax.set_xlim([-126, -112])\n",
        "plt.scatter(validation_examples[\"longitude\"],\n",
        "            validation_examples[\"latitude\"],\n",
        "            cmap=\"coolwarm\",\n",
        "            c=validation_examples[\"median_house_value\"] / validation_examples[\"median_house_value\"].max())\n",
        "\n",
        "_ = plt.plot()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ftNnIpjRG3cb"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 5: Re-train a Model (1 point) \n",
        "\n",
        "You may need to adjust your hyperparameter but use the same feature. Again, only use the training and validation data to do this portion."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9TP6SfVu9hy6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NUMERICAL_FEATURES = [\"households\"]\n",
        "LABEL = \"median_house_value\"\n",
        "\n",
        "LEARNING_RATE = 0.005\n",
        "STEPS = 50\n",
        "\n",
        "linear_regressor = define_linear_regression_model(learning_rate = LEARNING_RATE)\n",
        "linear_regressor = train_model(linear_regressor, steps=STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-BLiPNWWQRBg"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 6: Compute the Test Error for Your New Model (1/2 point)\n",
        "\n",
        "Compute the test error and then answer the question below."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uHhGrJIf9ndt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Fill this in"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "o3LscfZ8qinL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "How do the training error, validation error and test error now compare to each\n",
        "other?\n",
        "\n",
        "ANSWER:\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}